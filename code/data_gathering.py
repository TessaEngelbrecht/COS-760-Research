# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HWsO7WWTqdEdBYGmZkKdYwK1uM9x2EM2
"""

!pip install pandas --quiet

import pandas as pd

# âœ… URL to the isiZulu titles dataset
url = "https://raw.githubusercontent.com/dsfsi/za-isizulu-siswati-news-2022/v0.9.5/data/isizulu-titles-all-categories.csv"

# Load only the 'title' column
df = pd.read_csv(url)

# Make sure it's clean
df = df[["title"]].dropna()
df["text"] = df["title"].str.strip()
df = df[["text"]]
df["label"] = 0
df["language"] = "isiZulu"

# Save to new CSV
df.to_csv("human_isizulu.csv", index=False)

print(f"âœ… Saved {len(df)} human isiZulu titles to 'human_isizulu.csv'")
df.head()

from google.colab import files
files.download("human_isizulu.csv")

!pip install pandas --quiet
import pandas as pd
import zipfile
import os
import requests

# âœ… Step 1: Download isiZulu NER dataset from SADiLaR
url = "https://repo.sadilar.org/bitstream/handle/20.500.12185/319/nchlt_isizulu_named_entity_annotated_corpus.zip?sequence=3&isAllowed=y"
zip_path = "isizulu_ner.zip"

print("ðŸ“¥ Downloading isiZulu NER corpus...")
response = requests.get(url)
with open(zip_path, "wb") as f:
    f.write(response.content)

# âœ… Step 2: Extract ZIP to folder
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("isizulu_ner")

# âœ… Correct file path inside extracted folder
txt_path = "isizulu_ner/NCHLT isiZulu Named Entity Annotated Corpus/Dataset.NCHLT-II.zu.NER.Full.txt"

# âœ… Step 3: Parse tokens into sentences
sentences = []
tokens = []

with open(txt_path, "r", encoding="utf-8") as f:
    for line in f:
        line = line.strip()
        if not line:
            if tokens:
                sentence = " ".join(tokens)
                sentences.append(sentence)
                tokens = []
        else:
            token, *_ = line.split("\t")
            tokens.append(token)

# âœ… Add final sentence if still in buffer
if tokens:
    sentences.append(" ".join(tokens))

# âœ… Step 4: Save to CSV
df = pd.DataFrame({"text": sentences})
df["label"] = 0
df["language"] = "isiZulu"
df.to_csv("human_isizulu_ctext.csv", index=False)

print(f"âœ… Saved {len(df)} human-written isiZulu sentences to 'human_isizulu_ctext.csv'")
df.head()

from google.colab import files
files.download("human_isizulu_ctext.csv")

import pandas as pd

# Load both CSVs
df_ctext = pd.read_csv("human_isizulu_ctext.csv")
df_other = pd.read_csv("human_isizulu.csv")  # the smaller DSFSI one

# Combine the two
combined_df = pd.concat([df_ctext, df_other], ignore_index=True)

# Optional: Remove duplicates based on text content
combined_df = combined_df.drop_duplicates(subset="text").reset_index(drop=True)

# Save to new CSV
combined_df.to_csv("human_isizulu_combined.csv", index=False)

print(f"âœ… Combined dataset saved as 'human_isizulu_combined.csv' with {len(combined_df)} unique entries.")
combined_df.head()

from google.colab import files
files.download("human_isizulu_combined.csv")

import pandas as pd
import random

# Basic isiZulu sentence patterns (machine-style)
subjects = ["Umfundi", "Uhulumeni", "Abafundi", "Isikole", "Isisebenzi", "Uthisha", "Uzakwethu", "Uphenyo", "Isinqumo", "Uhlelo"]
verbs = ["uthumele", "uqale", "uphothule", "wakhipha", "uphendule", "uhlaziye", "uvumile", "ukwazile", "wahlulele", "ucwaninge"]
objects = ["incwadi", "uhlolo", "umsebenzi", "umbiko", "inkulumo", "iphrojekthi", "impendulo", "idatha", "isifundo", "uhlu"]

# Generate 1000 synthetic sentences
sentences = []
for _ in range(1000):
    sentence = f"{random.choice(subjects)} {random.choice(verbs)} {random.choice(objects)}."
    sentences.append({
        "text": sentence,
        "label": 1,
        "language": "isiZulu"
    })

# Convert to DataFrame and save
df_machine = pd.DataFrame(sentences)
df_machine.to_csv("machine_isizulu_generated.csv", index=False)

print("âœ… Generated 1000 AI-style isiZulu sentences as 'machine_isizulu_generated.csv'")
df_machine.head()

from google.colab import files
files.download("machine_isizulu_generated.csv")

import pandas as pd
import random

subjects = [
    "Umfundi", "Uhulumeni", "Isikole", "Uthisha", "Isisebenzi", "Iphrojekthi entsha",
    "Abafundi abaningi", "Uphenyo lwakamuva", "Uhlelo olusha", "Isikhulu"
]

verbs = [
    "uhlaziye", "uqale", "uphothule", "wakhipha", "uvumele", "uxhumane", "wenze",
    "uhlolile", "ucwaninge", "uhlomule", "uxoxile"
]

objects = [
    "uhlolo olubalulekile", "umbiko wesifundo esisha", "inkulumo mayelana necebo",
    "umsebenzi wephrojekthi", "inqubomgomo yomphakathi", "uhlelo lwesikole olusha",
    "indlela yokusebenza entsha", "amathuba emfundo", "ubufakazi besifundo",
    "uhlelo lokusebenza lwezisebenzi", "ulwazi olusha oluqukethwe ebhukwini"
]

connectors = [
    "ngaphandle kwezinkinga", "ngokuhambisana nezimiso", "ngezinsuku ezintathu ezedlule",
    "emhlanganweni obalulekile", "ngenhloso yokuthuthukisa ulwazi",
    "ezingeni eliphakeme", "njengoba bekulindelekile", "ngaphambi kokuphela kwesonto"
]

def generate_sentence():
    s = random.choice(subjects)
    v = random.choice(verbs)
    o = random.choice(objects)
    add = random.choice(connectors)

    # Ensure sentence is 6â€“13 words
    words = f"{s} {v} {o} {add}.".split()
    if len(words) < 6:
        return generate_sentence()
    if len(words) > 13:
        return generate_sentence()
    return " ".join(words)

# Generate 1000 valid-length machine-style isiZulu sentences
sentences = []
while len(sentences) < 2000:
    sentence = generate_sentence()
    if sentence not in sentences:
        sentences.append(sentence)

# Format for CSV
df = pd.DataFrame({
    "text": sentences,
    "label": 1,
    "language": "isiZulu"
})

# Save
df.to_csv("machine_isizulu_generated.csv", index=False)
print(f"âœ… 1000 AI-style isiZulu sentences saved to 'machine_isizulu_generated.csv'")
df.head()

from google.colab import files
files.download("machine_isizulu_generated.csv")

import pandas as pd
import random

# New vocabulary (no repeats from previous batch)
subjects = [
    "Isikhungo", "Ithimba lezentuthuko", "Uphenyo lwesayensi", "Inhlangano kazwelonke",
    "Abaholi bezemfundo", "Ibhodi labaphenyi", "Uhlelo lwezokuxhumana",
    "Abaxhumanisi bephrojekthi", "Izikhulu zikahulumeni", "Umsebenzi wezandla"
]

verbs = [
    "kubonise", "kuxhumanise", "kwethule", "kuthuthukise", "kwandise", "kuphakamise",
    "kuphumeleliswe", "kwamukele", "kwahlaziye", "kwatholakale"
]

objects = [
    "uhlaka lokusebenza lwezemfundo", "umbiko wokugcina", "indlela yokuphatha kahle",
    "uhlelo lwezokuxhumana", "izindlela ezintsha zokuqeqesha", "umgomo omusha",
    "amacebo ahlelekile", "imibiko yezinsuku ezedlule", "izinqubomgomo zesikhashana",
    "amathuba okuhlola izindlela ezintsha", "ubuchwepheshe obusetshenziswa ezifundweni"
]

connectors = [
    "ngaphansi kokwengamela okuqinile", "ngezinga eliyanelisayo", "ngokwemigomo esemthethweni",
    "ekubonisaneni okuqhubekayo", "ezimisweni eziphakeme", "ngendlela engaguquki",
    "ngokuhlonishwa kwemigomo", "ngokuhambisana nesikhathi", "emhlanganweni wamahhala",
    "ngokwesivumelwano esibhalwe phansi"
]

def generate_new_sentence(existing):
    while True:
        s = random.choice(subjects)
        v = random.choice(verbs)
        o = random.choice(objects)
        a = random.choice(connectors)
        sentence = f"{s} {v} {o} {a}.".strip()
        words = sentence.split()
        if 6 <= len(words) <= 13 and sentence not in existing:
            return sentence

# Generate 1000 new sentences
sentences = set()
while len(sentences) < 2000:
    new_sentence = generate_new_sentence(sentences)
    sentences.add(new_sentence)

# Convert to DataFrame
df2 = pd.DataFrame({
    "text": list(sentences),
    "label": 1,
    "language": "isiZulu"
})

# Save to CSV
df2.to_csv("machine_isizulu_generated_batch2.csv", index=False)
print("âœ… Another 1000 distinct AI-style isiZulu sentences saved to 'machine_isizulu_generated_batch2.csv'")
df2.head()

from google.colab import files
files.download("machine_isizulu_generated_batch2.csv")

import pandas as pd
import random

# Names and numerical variations
names = ["uSipho", "uThandi", "uMnu Dlamini", "uNkk Zuma", "uNomsa", "uLungelo", "uSibusiso", "uZanele", "uKabelo", "uAndile"]
numerics = ["ngo-2024", "ngo-2023", "izinsuku eziyishumi", "amaviki amabili", "unyaka odlule", "ngenyanga edlule", "ngeSonto eledlule"]

subjects = [
    "Uphenyo lwezibalo", "Isimemezelo esisemthethweni", "Icala elibhalwe ngo-2024",
    "Isifundo sikaProf Mthembu", "Inkulumo ka", "Ucingo oluvela ku", "Umbiko wango",
    "Isimemezelo sikahulumeni", "Ukuhlolwa kwangempela", "Inqubomgomo entsha"
]

verbs = [
    "luchaze", "lwabonisa", "lwamemezela", "lwavunywa", "lwakhiwe",
    "lwasetshenziswa", "luhlanganiswe", "luhlaziywe", "lwafakwa", "luhlelwe"
]

objects = [
    "indlela yokubala izindleko", "uhlu lwezinto ezisemqoka", "umhlahlandlela womthetho",
    "imiphumela yokuhlolwa", "ulwazi lwezindaba zakamuva", "inqubomgomo yomphakathi",
    "uhlelo lokulawulwa kwezinhlelo", "amagama ababhali", "ulwazi oludingekayo",
    "uhlu lwezinqubomgomo ezintsha", "amarekhodi aphathelene nezicelo"
]

connectors = [
    "ngokwesicelo esisemthethweni", "phambi kwekomidi", "emhlanganweni kaMasipala",
    "emhlanganweni wesifunda", "ngokwemigomo ye-Act 53", "ngaphansi kukaHulumeni omusha",
    "kusukela ngoJanuwari", "emhlanganweni wabaphathi abakhulu",
    "ngesicelo sikaNkk Zuma", "ngenhloso yokwenza lula izinto"
]

# Combine parts into sentences
def generate_sentence(existing_sentences):
    while True:
        name = random.choice(names)
        number = random.choice(numerics)
        s = f"{random.choice(subjects)} {name}" if random.random() < 0.4 else random.choice(subjects)
        v = random.choice(verbs)
        o = random.choice(objects)
        conn = random.choice(connectors)
        sentence = f"{s} {v} {o} {number} {conn}.".strip()
        words = sentence.split()
        if 6 <= len(words) <= 13 and sentence not in existing_sentences:
            return sentence

# Generate 1000 unique new sentences
sentences = set()
while len(sentences) < 2000:
    new_sentence = generate_sentence(sentences)
    sentences.add(new_sentence)

# Create DataFrame
df3 = pd.DataFrame({
    "text": list(sentences),
    "label": 1,
    "language": "isiZulu"
})

# Save
df3.to_csv("machine_isizulu_generated_batch3.csv", index=False)
print("âœ… Saved 1000 new AI-style isiZulu sentences to 'machine_isizulu_generated_batch3.csv'")
df3.head()

from google.colab import files
files.download("machine_isizulu_generated_batch3.csv")

import pandas as pd
import random

# Place names
places = [
    "eGoli", "KwaMashu", "Mthatha", "Durban", "Pretoria", "Lusikisiki",
    "Bhisho", "Ulundi", "eThekwini", "Polokwane", "Bloemfontein", "Soweto"
]

# Religious themes
religious_starts = [
    "Umfundisi", "Ibandla elikhulu", "Isikhungo senkolo", "Abakhulekeli", "Inkulumo yobuKristu",
    "Umthandazo wabafundi", "Ivangeli likaJesu", "Inkolo yesintu", "Umhlangano wamaKrestu",
    "Izifundo zikaPastor Dube"
]

religious_verbs = [
    "wathandazela", "wamemezela", "wavusa ithemba", "wakhuluma", "wacela ukuthula",
    "wahlela umsebenzi womoya", "wavumelana nabafundi", "waphawula ngezinkolelo",
    "wahlanganisa abalandeli", "wathumela umyalezo"
]

religious_objects = [
    "ngesonto eKwaMashu", "ngokholo oluqinile", "emhlanganweni wango-2023",
    "ezindleleni zokuphila ezingcwele", "ngemfundiso kaThixo",
    "emhlanganweni waseSoweto", "emithandazweni yomphakathi",
    "kuvangeli olutsha", "kuhlelo lokuhlengwa", "emathandweni angcwele"
]

connectors = [
    "ngokwesikhathi esinqunyiwe", "ezinsukwini eziyishumi ezedlule",
    "njengoba bekubhalwe encwadini", "phambi kwesonto",
    "ngeSonto ekuseni", "ngaphansi kwengalo kaNkulunkulu",
    "njengokwesimemezelo sabefundisi", "ngokuhlanganyela nabakhulekeli"
]

# Build more diverse structures by mixing place/religion variations
def generate_sentence(existing_sentences):
    if random.random() < 0.6:
        # religious sentence
        s = random.choice(religious_starts)
        v = random.choice(religious_verbs)
        o = random.choice(religious_objects)
        conn = random.choice(connectors)
    else:
        # place-based sentence
        s = f"UMasipala wase-{random.choice(places)}"
        v = random.choice([
            "umemezele", "uhambise", "wethule", "uqalile",
            "wenze uhlelo olusha", "waphawula", "wethule umbiko"
        ])
        o = random.choice([
            "uhlelo lokuthuthukisa umphakathi", "umbiko wesabelomali",
            "indlela yokuxhumana", "izinsiza zabantu", "imisebenzi yokwakha",
            "inqubomgomo yentuthuko", "indlela yokusebenza kahulumeni"
        ])
        conn = random.choice(connectors)

    sentence = f"{s} {v} {o} {conn}."
    words = sentence.split()
    if 6 <= len(words) <= 13 and sentence not in existing_sentences:
        return sentence
    else:
        return generate_sentence(existing_sentences)

# Generate 1000 unique sentences
sentences = set()
while len(sentences) < 2000:
    new_sentence = generate_sentence(sentences)
    sentences.add(new_sentence)

# Convert to DataFrame
df4 = pd.DataFrame({
    "text": list(sentences),
    "label": 1,
    "language": "isiZulu"
})

# Save
df4.to_csv("machine_isizulu_generated_batch4.csv", index=False)
print("âœ… Saved 1000 new AI-style isiZulu sentences with place & religious context to 'machine_isizulu_generated_batch4.csv'")
df4.head()

from google.colab import files
files.download("machine_isizulu_generated_batch4.csv")

import pandas as pd
import random

# Animal and object vocabulary
animals = ["inyamazane", "ihhashi", "inkukhu", "ingulube", "ibhubesi", "impisi", "inyoni", "inhlwathi"]
objects = ["ibhayisikili", "ithebula", "ibhodwe", "isikole", "umakhalekhukhwini", "umshini wokupheka", "ikhabethe", "isambulela"]

subjects = [
    "Umfundi omusha", "Uthisha waseNanda", "Isisebenzi esiqavile", "Abazali bezingane",
    "Abantu bendawo", "Uhlelo lwezemvelo", "Uprofesa wakwaZulu", "Umphathi wepulazi"
]

verbs = [
    "wabona", "wathumela", "wahlela", "wahlola", "waklame", "wathengisela",
    "wafundisa", "wacwaninga", "wasebenzisa", "wazisa", "wakwazi ukusebenzisa", "wanikeza"
]

locations = [
    "esikoleni saseMthatha", "emapulazini aseKwaMashu", "ehhovisi elikhulu",
    "endaweni yomphakathi", "esibhedlela esiseduze", "emhlanganweni waseThekwini",
    "endlini yaseLusikisiki", "eholweni labafundi"
]

connectors = [
    "ngendlela ecacile nenembayo", "ngokuhambisana nezindinganiso", "ngaphandle kobunzima obukhulu",
    "njengoba kuchaziwe ngaphambilini", "ngenhlonipho ephelele",
    "ngezinjongo zokuthuthukisa ulwazi", "ngokusebenzisa izinsiza ezikhona",
    "ngemibandela ethile esinqunyiwe", "kanye nabanye abafundi abakhulu"
]

def generate_sentence(existing_sentences):
    while True:
        s = random.choice(subjects)
        v = random.choice(verbs)
        a = random.choice(animals)
        o = random.choice(objects)
        l = random.choice(locations)
        c = random.choice(connectors)

        # Optional prepositional phrase
        extra = random.choice([f"ebusika ngo-2023", f"ngemuva kwesikhathi eside", f"ngaphakathi kwekilasi", "eholidini eledlule"])

        sentence = f"{s} {v} {a} kanye ne-{o} {l} {c} {extra}."
        words = sentence.split()
        if 10 <= len(words) <= 20 and sentence not in existing_sentences:
            return sentence

# Generate 1000 unique long-form sentences
sentences = set()
while len(sentences) < 2000:
    new_sentence = generate_sentence(sentences)
    sentences.add(new_sentence)

# Create DataFrame
df5 = pd.DataFrame({
    "text": list(sentences),
    "label": 1,
    "language": "isiZulu"
})

# Save to CSV
df5.to_csv("machine_isizulu_generated_batch5.csv", index=False)
print("âœ… Saved 1000 AI-style isiZulu sentences (10â€“20 words, with animals/objects) to 'machine_isizulu_generated_batch5.csv'")
df5.head()

from google.colab import files
files.download("machine_isizulu_generated_batch4.csv")

import pandas as pd

# Load each batch
batch1 = pd.read_csv("machine_isizulu_generated.csv")
batch2 = pd.read_csv("machine_isizulu_generated_batch2.csv")
batch3 = pd.read_csv("machine_isizulu_generated_batch3.csv")
batch4 = pd.read_csv("machine_isizulu_generated_batch4.csv")
batch5 = pd.read_csv("machine_isizulu_generated_batch5.csv")

# Combine them
combined_machine_df = pd.concat([batch1, batch2, batch3, batch4, batch5], ignore_index=True)

# Optional: Remove duplicates just in case
combined_machine_df = combined_machine_df.drop_duplicates(subset="text").reset_index(drop=True)

# Save to a single machine file
combined_machine_df.to_csv("machine_isizulu_combined.csv", index=False)

print(f"âœ… Combined machine dataset saved as 'machine_isizulu_combined.csv' with {len(combined_machine_df)} unique entries.")
combined_machine_df.head()

from google.colab import files
files.download("machine_isizulu_combined.csv")



# Move file to your Drive for persistence
!mv training_data.csv /content/drive/MyDrive/ColabData/training_data.csv

import pandas as pd

# Upload files from your device first (explained below ðŸ‘‡)

# Load both CSVs
df_human = pd.read_csv("human_isizulu_combined.csv")
df_machine = pd.read_csv("machine_isizulu_combined.csv")

# Optional: drop duplicates
df_combined = pd.concat([df_human, df_machine], ignore_index=True).drop_duplicates(subset="text").reset_index(drop=True)

# Shuffle the dataset (optional but recommended)
df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)

# Save to one training file
df_combined.to_csv("training_data.csv", index=False)

print(f"âœ… Combined dataset saved as 'training_data.csv' with {len(df_combined)} rows.")
df_combined.head()

from google.colab import files
files.download("training_data.csv")

from datasets import load_dataset
import pandas as pd

# Load isiXhosa data from MasakhaPOS
dataset = load_dataset("masakhane/masakhapos", name="xho", split="train")

# Convert tokens to sentences
sentences = [" ".join(example["tokens"]) for example in dataset]

# Build dataframe
df_xho = pd.DataFrame({
    "text": sentences,
    "label": 0,
    "language": "isiXhosa"
})

# Save to CSV
df_xho.to_csv("human_isixhosa_masakhapos.csv", index=False)

print("âœ… Saved as 'human_isixhosa_masakhapos.csv'")

from google.colab import files
files.download("human_isixhosa_masakhapos.csv")

from datasets import load_dataset
import pandas as pd

# Load isiXhosa portion of MasakhaNER2.0
dataset = load_dataset("masakhane/masakhaner2", name="xho", split="train")

# Convert tokens to full sentences
sentences = [" ".join(example["tokens"]) for example in dataset]

# Build DataFrame
df_xho_ner = pd.DataFrame({
    "text": sentences,
    "label": 0,
    "language": "isiXhosa"
})

# Save to CSV
df_xho_ner.to_csv("human_isixhosa_masakhaner2.csv", index=False)

print("âœ… Saved to 'human_isixhosa_masakhaner2.csv'")

from google.colab import files
files.download("human_isixhosa_masakhaner2.csv")

from google.colab import files
uploaded = files.upload()  # Select `xho-za_web_2020_30K-sentences.txt` when prompted

import pandas as pd

# Load and clean the file (removes numbers and keeps sentences only)
lines = []
with open("xho-za_web_2020_30K-sentences.txt", "r", encoding="utf-8") as file:
    for line in file:
        # Split on the first tab, remove extra whitespace
        parts = line.strip().split("\t", 1)
        if len(parts) == 2:
            lines.append(parts[1].strip())

# Convert to DataFrame
df = pd.DataFrame(lines, columns=["text"])
df["label"] = 0  # 0 = Human

# Save to CSV
df.to_csv("human_isixhosa_leipzig.csv", index=False)
print("âœ… Saved as human_isixhosa_leipzig.csv")

from google.colab import files
files.download("human_isixhosa_leipzig.csv")

import pandas as pd

# Load each dataset
df_leipzig = pd.read_csv("human_isixhosa_leipzig.csv")
df_ner = pd.read_csv("human_isixhosa_masakhaner2.csv")
df_pos = pd.read_csv("human_isixhosa_masakhapos.csv")

# Combine them
combined_df = pd.concat([df_leipzig, df_ner, df_pos], ignore_index=True)

# Drop duplicates just in case
combined_df.drop_duplicates(subset="text", inplace=True)

# Save to a new CSV
combined_df.to_csv("human_isixhosa_combined.csv", index=False)

print(f"âœ… Combined dataset saved with {len(combined_df)} rows as 'human_isixhosa_combined.csv'")

from google.colab import files
files.download("human_isixhosa_combined.csv")

import csv
import random

# Define a small pool of isiXhosa sentence fragments to generate varied sentences
subjects = ["Umama", "Ubawo", "Abantwana", "Utitshala", "Inkosikazi", "Amadoda", "Iinkwenkwezi", "Iintsapho", "Izilwanyana"]
verbs = ["uyapheka", "bakhawuleza", "bahlala", "badlala", "bafunda", "bayakhumbula", "babhalela", "bayanceda", "bayathenga"]
objects = ["etafileni", "ekhaya", "evenkileni", "esikolweni", "emhlabeni", "ngezandla zabo", "ngethemba elikhulu", "ngendlela elungileyo", "ngeemvakalelo ezinzulu"]
extras = [
    "ngexesha lemvula enkulu", "kwixesha lasekuseni", "ngokuhlwa rhoqo", "ngesingxobo semali",
    "ukusuka eKapa", "ngaphandle kwefama", "ekupheleni kweveki", "emva kweentsuku ezimbini", "ngomvulo ngentsimbi yesithathu"
]

# Function to create a sentence between 8 and 15 words
def generate_sentence():
    length = random.randint(8, 15)
    sentence = []
    while len(sentence) < length:
        part = random.choice([random.choice(subjects), random.choice(verbs), random.choice(objects), random.choice(extras)])
        sentence.append(part)
    return " ".join(sentence[:length])

# Generate 1000 sentences
data = [{"text": generate_sentence(), "label": 1, "language": "isiXhosa"} for _ in range(2000)]

# Save to CSV

filename = "machine_isixhosa_generated.csv"
with open(filename, mode="w", encoding="utf-8", newline="") as file:
    writer = csv.DictWriter(file, fieldnames=["text", "label", "language"])
    writer.writeheader()
    writer.writerows(data)


filename

from google.colab import files
files.download("machine_isixhosa_generated.csv")

import random
import pandas as pd

# Predefined lists to enrich sentence diversity
names = ["Lindiwe", "Thabo", "Sipho", "Nomsa", "Andile", "Zanele", "Kwanda", "Bongiwe", "Ayanda", "Noluthando"]
places = ["eGqeberha", "eBhisho", "eKapa", "eMthatha", "eJohannesburg", "eDurban", "eEast London", "eKimberley", "ePolokwane"]
objects = ["ibhola", "incwadi", "imoto", "ikhompyutha", "ifowuni", "ithelevishini", "itafile", "isitulo", "ikhethini"]
actions = ["wathenga", "wahamba", "wafunda", "wabhala", "waxoxa", "wabukela", "wadlala", "wathengisa", "wabiza", "waphendula"]
connectors = ["kwi", "naka", "njengoba", "ngaphambi", "ngemva", "ngenxa", "kunye", "kodwa", "ngaphandle", "nangona"]

# Generate 1000 varied isiXhosa sentences
def generate_sentence():
    length = random.randint(10, 20)
    sentence = []

    while len(sentence) < length:
        choice = random.choice(["name_place_action", "object_action", "connector_phrase"])
        if choice == "name_place_action":
            sentence.append(random.choice(names))
            sentence.append(random.choice(actions))
            sentence.append(random.choice(places))
        elif choice == "object_action":
            sentence.append("i" + random.choice(objects))
            sentence.append(random.choice(actions))
        elif choice == "connector_phrase":
            sentence.append(random.choice(connectors))

    sentence = sentence[:length]
    return " ".join(sentence).strip().capitalize() + "."

# Create the dataset
data = {
    "text": [generate_sentence() for _ in range(2000)],
    "label": [1] * 2000,  # 1 for machine-generated
    "language": ["xh"] * 2000  # xh for isiXhosa
}

df = pd.DataFrame(data)
file_path = "machine_isixhosa_generated_batch2.csv"
df.to_csv(file_path, index=False)

file_path

from google.colab import files
files.download("machine_isixhosa_generated_batch2.csv")

import pandas as pd
import random

# Define lists for names, places, and objects to enrich sentence content
names = ["Sibongile", "Lukhanyo", "Ayanda", "Zandile", "Themba", "Nosipho", "Thabo", "Naledi", "Mandisa", "Sipho"]
places = ["eKapa", "eBhayi", "eGcuwa", "eMthatha", "eQonce", "eSoweto", "ePitoli", "eMonti", "eGoli", "eKhayelitsha"]
objects = ["ikhompyutha", "ifowuni", "incwadi", "itafile", "ibhasi", "imoto", "isikolo", "ivenkile", "indlu", "isixhobo"]

def generate_isixhosa_sentence():
    length = random.randint(10, 20)
    name = random.choice(names)
    place = random.choice(places)
    obj = random.choice(objects)
    number = str(random.randint(1, 100))

    templates = [
        f"{name} uye waya {place} nge-{obj} kunye nabahlobo bakhe abangama-{number}.",
        f"Kwakukho intlanganiso enkulu ebanjelwe {place} apho u{name} wayethetha nge-{obj}.",
        f"Abantwana abangama-{number} babhaliselwe izifundo ezikhethekileyo e{place}.",
        f"{name} ufumene amanqaku aphezulu kwi-{obj} yakhe yesi-{number}.",
        f"I-{obj} entsha ka{name} ithengwe {place} ngemali eyi-{number} yeerandi.",
        f"Ngomhla we-{number}, u{name} uye waya e{place} ngethuba leholide.",
        f"Abafundi base-{place} bafunde ngendlela yokusebenzisa i-{obj} enikwe ngu{name}.",
        f"U{name} wathumela i-imeyile e{place} efuna ulwazi nge-{obj}.",
        f"{number} abantu babekwiqela lika{name} eliye e{place}.",
        f"Isikolo esikufutshane ne-{place} safumana izipho ezili-{number} ezivela ku{name}.",
    ]

    return random.choice(templates)

# Generate 1000 sentences
data = {
    "text": [generate_isixhosa_sentence() for _ in range(2000)],
    "label": [1] * 2000,  # 1 = machine-generated
    "language": ["xh"] * 2000
}

df_xhosa_ai_batch3 = pd.DataFrame(data)
file_path = "machine_isixhosa_generated_batch3.csv"
df_xhosa_ai_batch3.to_csv(file_path, index=False)

file_path

from google.colab import files
files.download("machine_isixhosa_generated_batch3.csv")

import pandas as pd
import random

# isiXhosa sentence components
names = ["uSipho", "uThandi", "uLwazi", "uNomsa", "uSizwe", "uBongi", "uAndile", "uZanele", "uKhaya", "uLindiwe"]
places = ["eKapa", "eGoli", "eBhayi", "eMthatha", "eQonce", "eMonti", "eDurban", "eSoweto", "ePolokwane", "eTshwane"]
objects = ["incwadi", "ikhompyutha", "ifowuni", "imoto", "ibhayisekile", "ithelevishini", "umabonwakude", "isikhululo", "indlu", "iipensile"]
verbs = ["wathenga", "wavula", "wahamba", "wathetha", "wabona", "wakha", "wabhala", "wathandaza", "wadlala", "wafunda"]
extras = ["ngokukhawuleza", "ngaphakathi kweveki", "ngempelaveki", "kusasa", "ngokuhlwa", "ngonyaka ophelileyo", "kwisikhumbuzo", "ngomhla wesine", "ngoMvulo", "nangona bekukho imvula"]

def generate_sentence():
    length = random.randint(14, 25)
    sentence = []
    while len(sentence) < length:
        chunk = random.choice([
            f"{random.choice(names)} {random.choice(verbs)} {random.choice(objects)} {random.choice(places)}",
            f"{random.choice(places)} {random.choice(verbs)} {random.choice(objects)}",
            f"{random.choice(names)} {random.choice(verbs)} {random.choice(objects)} {random.choice(extras)}",
            f"{random.choice(names)} {random.choice(verbs)} {random.choice(objects)} kwindawo ethiwa {random.choice(places)}"
        ])
        sentence += chunk.split()
    return " ".join(sentence[:length]).strip().capitalize() + "."

# Generate 1000 unique sentences
machine_data = {
    "text": [generate_sentence() for _ in range(2000)],
    "label": [1] * 2000,
    "language": ["xh"] * 2000
}

df_machine = pd.DataFrame(machine_data)
file_path = "machine_isixhosa_generated_batch4.csv"
df_machine.to_csv(file_path, index=False)

file_path

from google.colab import files
files.download("machine_isixhosa_generated_batch4.csv")

import random
import pandas as pd

# Word banks for realism
names = ["Sipho", "Thandi", "Lerato", "Vusi", "Nomsa", "Bongani", "Zanele", "Andile", "Lindiwe", "Sibusiso"]
places = ["eKapa", "eGoli", "eMonti", "eBhayi", "eMthatha", "eSoweto", "eTshwane", "ePolokwane", "eDurban", "eBloemfontein"]
objects = ["itafile", "ikhompyutha", "imoto", "ifowuni", "ibhayisikile", "iTV", "incwadi", "i-backpack", "ikhamera", "i-drone"]
actions = ["ufunda", "uyasebenza", "uyaqhuba", "uyacula", "udlala", "ubhala", "uxoxa", "uyathengisa", "uyafota", "wenza umsebenzi"]
times = ["ngentsasa", "ngokuhlwa", "ngeholo", "ngempelaveki", "namhlanje", "izolo", "kusasa", "ngale veki", "nangomso", "kwiveki ezayo"]

# Sentence generation function
def generate_realistic_xhosa_sentence(min_len, max_len):
    sentence_len = random.randint(min_len, max_len)
    sentence = []

    while len(sentence) < sentence_len:
        word_source = random.choices(
            [names, places, objects, actions, times],
            weights=[3, 3, 2, 4, 2],
            k=1
        )[0]
        word = random.choice(word_source)
        sentence.append(word)

    return " ".join(sentence).capitalize() + "."

# Generate 1000 sentences
xhosa_sentences = [generate_realistic_xhosa_sentence(8, 30) for _ in range(2000)]

# Create dataframe
df = pd.DataFrame({
    "text": xhosa_sentences,
    "label": [1] * len(xhosa_sentences),
    "language": ["xh"] * len(xhosa_sentences)
})

# Save CSV
output_path = "machine_isixhosa_generated_batch5.csv"
df.to_csv(output_path, index=False)

output_path

from google.colab import files
files.download("machine_isixhosa_generated_batch5.csv")

import pandas as pd
import random

# Lists of contextual words for realism
animal_names = ["indlovu", "ingwe", "ibhere", "inyathi", "ingulule", "unonkala", "ikati", "inja", "igusha", "inkomo"]
places = ["eGcuwa", "eMthatha", "eBhisho", "eQonce", "eKapa", "eDurban", "eSoweto", "ePolokwane", "eMpumalanga"]
people_names = ["uThabo", "uNomsa", "uSipho", "uZanele", "uLindiwe", "uBongani", "uAndile", "uThemba", "uAyanda"]
activities = ["wavela", "wahamba", "wabonakala", "wathatha", "waziswa", "wafundisa", "wabhalisela", "wayokuthetha", "waxelela"]
misc_terms = ["indlu", "isikolo", "ibhasi", "iphulo", "uluntu", "intlanganiso", "imali", "umnyhadala", "indlela", "ithuba"]

def generate_sentence(min_len, max_len):
    length = random.randint(min_len, max_len)
    sentence = []
    while len(sentence) < length:
        category = random.choice(["animal", "place", "name", "activity", "misc"])
        if category == "animal":
            sentence.append(random.choice(animal_names))
        elif category == "place":
            sentence.append(random.choice(places))
        elif category == "name":
            sentence.append(random.choice(people_names))
        elif category == "activity":
            sentence.append(random.choice(activities))
        else:
            sentence.append(random.choice(misc_terms))
    # Capitalise first word and end with a full stop
    sentence[0] = sentence[0].capitalize()
    return " ".join(sentence) + "."

# Generate 1000 sentences
sentences = [generate_sentence(10, 40) for _ in range(2000)]

# Create DataFrame
df = pd.DataFrame({
    "text": sentences,
    "label": [1] * 2000,
    "language": ["xh"] * 2000
})

# Save to CSV
file_path = "machine_isixhosa_generated_batch6.csv"
df.to_csv(file_path, index=False)

file_path

from google.colab import files
files.download("machine_isixhosa_generated_batch6.csv")

import pandas as pd

# List of filenames (adjust this list if needed)
filenames = [
    "machine_isixhosa_generated.csv",
    "machine_isixhosa_generated_batch2.csv",
    "machine_isixhosa_generated_batch3.csv",
    "machine_isixhosa_generated_batch4.csv",
    "machine_isixhosa_generated_batch5.csv",
    "machine_isixhosa_generated_batch6.csv"
]

# Read and concatenate all CSVs
dfs = [pd.read_csv(file) for file in filenames]
combined_df = pd.concat(dfs, ignore_index=True)

# Save the combined CSV
combined_df.to_csv("machine_isixhosa_combined.csv", index=False)
print("âœ… Combined CSV saved as machine_isixhosa_combined.csv")

from google.colab import files
files.download("machine_isixhosa_combined.csv")

import pandas as pd

# Upload files from your device first (explained below ðŸ‘‡)

# Load both CSVs
df_human = pd.read_csv("human_isixhosa_combined.csv")
df_machine = pd.read_csv("machine_isixhosa_combined.csv")

# Optional: drop duplicates
df_combined = pd.concat([df_human, df_machine], ignore_index=True).drop_duplicates(subset="text").reset_index(drop=True)

# Shuffle the dataset (optional but recommended)
df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)

# Save to one training file
df_combined.to_csv("training_data_xhosa.csv", index=False)

print(f"âœ… Combined dataset saved as 'training_data.csv' with {len(df_combined)} rows.")
df_combined.head()

from google.colab import files
files.download("training_data_xhosa.csv")