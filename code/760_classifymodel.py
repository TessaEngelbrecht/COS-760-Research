# -*- coding: utf-8 -*-
"""760 ClassifyModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B9FMRyw1Krc6ac3YrS8t80GY-4IyEXd6
"""

!pip install transformers datasets scikit-learn
!pip install nlpaug
import torch
from torch import nn
from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, MarianMTModel, MarianTokenizer
from datasets import Dataset
import openai
import csv
import os
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from datasets import load_dataset
os.environ["WANDB_DISABLED"] = "true"
import nlpaug.augmenter.word as naw
from google.colab import drive
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
drive.mount('/content/drive')

model_name = "Davlan/afro-xlmr-large"

file_paths = {
    'xhosa_training': "/content/drive/MyDrive/Colab Notebooks/COS760 Project/training_data_xhosa.csv",
    'xhosa_augmented': "/content/drive/MyDrive/Colab Notebooks/COS760 Project/training_data_xhosa_augmented.csv",
    'zulu_training': "/content/drive/MyDrive/Colab Notebooks/COS760 Project/training_data_zulu.csv"
}
human = 0
machine = 1

# Load translation models
xh2en_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-xh-en")
xh2en_tok = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-xh-en")

en2xh_model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-en-xh")
en2xh_tok = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-xh")
syn_aug = naw.SynonymAug(aug_src="wordnet", aug_p=0.1)

# Function to back-translate a Xhosa sentence
def back_translate(text):
    # Xhosa → English
    encoded_xh = xh2en_tok(text, return_tensors="pt", truncation=True, padding=True)
    translated_en = xh2en_model.generate(**encoded_xh)
    english = xh2en_tok.decode(translated_en[0], skip_special_tokens=True)

    # English → Xhosa
    encoded_en = en2xh_tok(english, return_tensors="pt", truncation=True, padding=True)
    translated_xh = en2xh_model.generate(**encoded_en)
    xhosa_back = en2xh_tok.decode(translated_xh[0], skip_special_tokens=True)

    return {
        "original": text,
        "english": english,
        "back_translated": xhosa_back
    }

# df["label"].value_counts()

class AfroXLMRBinaryClassifier(nn.Module):
    def __init__(self, model_name, num_classes=2):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        self.classifier = nn.Linear(1024, num_classes)  # 1024 is die size van afro-xlmr-large se laaste layer

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        cls_token = outputs.last_hidden_state[:, 0, :]
        logits = self.classifier(cls_token)

        if labels is not None:
            loss_fn = nn.CrossEntropyLoss()
            loss = loss_fn(logits, labels)
            return {"loss": loss, "logits": logits}
        return {"logits": logits}

class CustomModelWrapper(nn.Module):
    def __init__(self, model_name, num_classes):
        super().__init__()
        self.model = AfroXLMRBinaryClassifier(model_name, num_classes=num_classes)

    def forward(self, input_ids=None, attention_mask=None, labels=None):
        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)


def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

tokenizer = AutoTokenizer.from_pretrained(model_name)

def augment_and_tokenize(example):

    try:
        aug_text = back_translate(example["text"])["back_translated"]
    except:
        aug_text = example["text"]
    return tokenizer(aug_text, padding="max_length", truncation=True, max_length=128)

def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128)

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/Colab Notebooks/COS760 Project/results",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=1,
    weight_decay=0.01,
    eval_strategy="steps",
    logging_strategy="steps",

    eval_steps=50,
    logging_steps=10,
    report_to="none",
    logging_dir="./logs",
    log_level="info",
)

df_xhosa_aug = pd.read_csv(file_paths['xhosa_augmented'])
# df_zulu = pd.read_csv(file_paths['zulu_training'])

def run_experiment(df, name, tokenize_fn, model_name, training_args):
    dataset = Dataset.from_pandas(df[["text", "label"]])
    dataset = dataset.train_test_split(test_size=0.1, shuffle=True)
    train_dataset = dataset["train"].map(tokenize_fn).remove_columns(["text"])
    test_dataset = dataset["test"].map(lambda x: tokenize_fn(x)).remove_columns(["text"])

    trainer = Trainer(
        model=CustomModelWrapper(model_name, 2),
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
    )

    trainer.evaluate()
    trainer.train()

    df_logs = pd.DataFrame(trainer.state.log_history)
    df_eval = df_logs[df_logs['eval_loss'].notnull()]
    df_train = df_logs[df_logs['loss'].notnull()]

    plt.figure(figsize=(10, 6))
    plt.plot(df_eval['step'], df_eval['eval_accuracy'], label='Eval Accuracy')
    plt.plot(df_eval['step'], df_eval['eval_loss'], label='Eval Loss')
    plt.plot(df_train['step'], df_train['loss'], label='Train Loss')
    plt.xlabel("Step")
    plt.ylabel("Metric")
    plt.title(f"Training & Evaluation Metrics: {name}")
    plt.grid(True)
    plt.legend()
    plt.savefig(f"/content/drive/MyDrive/Colab Notebooks/COS760 Project/results/{name.replace(' ', '_').lower()}.png")
    plt.close()

# import random
# from tqdm import tqdm

# from transformers import pipeline

# sampled_df = df_xhosa.sample(frac=0.05, random_state=42)
# indices = sampled_df.index.tolist()
# texts = sampled_df["text"].tolist()

# xh2en = pipeline("translation", model="Helsinki-NLP/opus-mt-xh-en", device=0)  # use GPU if available
# en2xh = pipeline("translation", model="Helsinki-NLP/opus-mt-en-xh", device=0)

# BATCH_SIZE = 16
# translated_texts = []

# for i in tqdm(range(0, len(texts), BATCH_SIZE), desc="Back-translating"):
#     batch = texts[i:i + BATCH_SIZE]
#     try:
#         en_batch = xh2en(batch, max_length=512)
#         en_texts = [item["translation_text"] for item in en_batch]

#         # English → Xhosa
#         xh_batch = en2xh(en_texts, max_length=512)
#         xh_texts = [item["translation_text"] for item in xh_batch]

#         translated_texts.extend(xh_texts)
#     except Exception as e:
#         translated_texts.extend(batch)  # fallback to original

# # Replace the original texts with back-translated ones
# df_xhosa.loc[indices, "text"] = translated_texts

# # Save to file
# df_xhosa.to_csv("training_data_xhosa_augmented.csv", index=False)

# for lang_name, df_lang in [("Xhosa", df_xhosa), ("Zulu", df_zulu), ("Both", pd.concat([df_xhosa, df_zulu], ignore_index=True))]:
#     for token_fn_name, token_fn in [("tokenize", tokenize), ("augment_and_tokenize", augment_and_tokenize)]:
#         run_name = f"{lang_name} - {token_fn_name}"
#         print(f"Running: {run_name}")
#         run_experiment(df_lang, run_name, token_fn, model_name, training_args)

print(f"Running: Xhosa - tokenize")
run_experiment(df_xhosa, "Xhosa - tokenize", tokenize, model_name, training_args)
# print(f"Running: Xhosa - augment_and_tokenize")
# run_experiment(df_xhosa, "Xhosa - augment_and_tokenize", augment_and_tokenize, model_name, training_args)
# print(f"Running: Zulu - tokenize")
# run_experiment(df_zulu, "Zulu - tokenize", tokenize, model_name, training_args)
# print(f"Running: Zulu - augment_and_tokenize")
# run_experiment(df_zulu, "Zulu - augment_and_tokenize", augment_and_tokenize, model_name, training_args)
# df_both = pd.concat([df_xhosa, df_zulu], ignore_index=True)
# print(f"Running: Both - tokenize")
# run_experiment(df_both, "Both - tokenize", tokenize, model_name, training_args)
# print(f"Running: Both - augment_and_tokenize")
# run_experiment(df_both, "Both - augment_and_tokenize", augment_and_tokenize, model_name, training_args)

trainer.model.eval()
inputs = tokenizer("Ngiyabona ukuthi izulu lizoshaya namhlanje.", return_tensors="pt")

device = next(trainer.model.parameters()).device
inputs = {k: v.to(device) for k, v in inputs.items()}

with torch.no_grad():
    logits = trainer.model(**inputs)["logits"]
    probs = torch.softmax(logits, dim=1)

    predicted_class = torch.argmax(probs).item()
    confidence = probs.squeeze().tolist()

    print(f"Predicted class: {predicted_class}")
    print(f"Confidence: Human (0) = {confidence[0]:.4f}, Machine (1) = {confidence[1]:.4f}")
    print("\n Warning: Because of the synthetic data and restricted language representation, this model might be biased. Use in human-supervised critical systems.")